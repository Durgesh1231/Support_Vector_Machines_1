{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9B_EK4Me9ix"
      },
      "outputs": [],
      "source": [
        "# Q1. What is the mathematical formula for a linear SVM?\n",
        "# The objective of a linear Support Vector Machine (SVM) is to find a hyperplane that best separates\n",
        "# two classes in a feature space. The equation of the hyperplane is:\n",
        "# w^T * x + b = 0, where:\n",
        "# w is the weight vector (normal vector to the hyperplane),\n",
        "# x is the feature vector,\n",
        "# b is the bias term.\n",
        "\n",
        "# Q2. What is the objective function of a linear SVM?\n",
        "# The objective function of a linear SVM is to maximize the margin (the distance between the hyperplane\n",
        "# and the closest data points from both classes). This can be mathematically formulated as:\n",
        "# Minimize (1/2) * ||w||^2, subject to the constraints that:\n",
        "# y_i * (w^T * x_i + b) â‰¥ 1 for all i, where:\n",
        "# y_i is the true label for the i-th training point (+1 or -1),\n",
        "# x_i is the feature vector for the i-th training point.\n",
        "\n",
        "# Q3. What is the kernel trick in SVM?\n",
        "# The kernel trick allows SVM to operate in higher-dimensional spaces without explicitly computing the\n",
        "# coordinates of the data in that space. Instead of computing the inner product in the higher-dimensional\n",
        "# space, we apply a kernel function to map the input space to a higher-dimensional space implicitly.\n",
        "# Common kernel functions include:\n",
        "# 1. Linear Kernel: K(x, y) = x^T * y\n",
        "# 2. Polynomial Kernel: K(x, y) = (x^T * y + c)^d\n",
        "# 3. Radial Basis Function (RBF) Kernel: K(x, y) = exp(-||x - y||^2 / 2 * sigma^2)\n",
        "\n",
        "# Q4. What is the role of support vectors in SVM? Explain with an example\n",
        "# Support vectors are the data points that lie closest to the decision boundary (hyperplane).\n",
        "# These points are critical for determining the optimal hyperplane, as the margin is defined by the\n",
        "# distance from these support vectors to the hyperplane. If support vectors are removed, the optimal\n",
        "# hyperplane may change, which can affect the classifier's performance.\n",
        "# Example: In a 2D feature space, imagine two classes, \"Class 1\" and \"Class 2\". The support vectors\n",
        "# are the points that are closest to the line separating the two classes. The distance from these\n",
        "# points to the hyperplane defines the margin.\n",
        "\n",
        "# Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM.\n",
        "# Hyperplane: A hyperplane is a decision boundary that separates data points belonging to different classes.\n",
        "# In a 2D feature space, it is a straight line.\n",
        "# Marginal plane: It is a parallel plane to the hyperplane and defines the margin. The margin is the distance\n",
        "# between the hyperplane and the nearest data points from each class.\n",
        "# Soft margin: In cases where data points cannot be perfectly separated (non-linearly separable), a soft margin\n",
        "# allows some misclassification to achieve a more generalized model.\n",
        "# Hard margin: A hard margin classifier is used when data is linearly separable and no misclassification is allowed.\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Q6. SVM Implementation through Iris dataset.\n",
        "# Load the Iris dataset from scikit-learn\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # Use only the first two features for visualization\n",
        "y = iris.target\n",
        "\n",
        "# We will only work with the first two classes (Setosa and Versicolor)\n",
        "X = X[y != 2]  # Remove Virginica class\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a linear SVM classifier\n",
        "svm = SVC(kernel='linear', C=1)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels on the testing set\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Compute the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of SVM on the test set: {accuracy:.2f}\")\n",
        "\n",
        "# Plot the decision boundaries of the trained model\n",
        "xx, yy = np.meshgrid(np.linspace(X[:, 0].min(), X[:, 0].max(), 100),\n",
        "                     np.linspace(X[:, 1].min(), X[:, 1].max(), 100))\n",
        "\n",
        "Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.coolwarm)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', cmap=plt.cm.coolwarm)\n",
        "plt.title(\"SVM Decision Boundary\")\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n",
        "\n",
        "# Try different values of the regularization parameter C\n",
        "for C_value in [0.1, 1, 10]:\n",
        "    svm = SVC(kernel='linear', C=C_value)\n",
        "    svm.fit(X_train, y_train)\n",
        "    y_pred = svm.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy with C={C_value}: {accuracy:.2f}\")\n",
        "\n",
        "# Bonus task: Implement a linear SVM from scratch\n",
        "\n",
        "# Step 1: Define a Linear SVM Class\n",
        "class LinearSVM:\n",
        "    def __init__(self, C=1.0, learning_rate=0.001, epochs=1000):\n",
        "        self.C = C  # Regularization parameter\n",
        "        self.learning_rate = learning_rate  # Learning rate\n",
        "        self.epochs = epochs  # Number of iterations\n",
        "        self.w = None  # Weight vector\n",
        "        self.b = None  # Bias term\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Initialize parameters\n",
        "        num_samples, num_features = X.shape\n",
        "        self.w = np.zeros(num_features)\n",
        "        self.b = 0\n",
        "\n",
        "        # Training loop\n",
        "        for _ in range(self.epochs):\n",
        "            for i in range(num_samples):\n",
        "                condition = y[i] * (np.dot(X[i], self.w) + self.b) >= 1\n",
        "                if condition:\n",
        "                    self.w -= self.learning_rate * (2 * self.C * self.w)\n",
        "                else:\n",
        "                    self.w -= self.learning_rate * (2 * self.C * self.w - np.dot(X[i], y[i]))\n",
        "                    self.b -= self.learning_rate * y[i]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w) + self.b)\n",
        "\n",
        "# Step 2: Train and evaluate the custom Linear SVM\n",
        "svm_from_scratch = LinearSVM(C=1.0)\n",
        "svm_from_scratch.fit(X_train, y_train)\n",
        "y_pred_scratch = svm_from_scratch.predict(X_test)\n",
        "accuracy_scratch = accuracy_score(y_test, y_pred_scratch)\n",
        "print(f\"Accuracy of the custom Linear SVM: {accuracy_scratch:.2f}\")\n",
        "\n",
        "# Compare with scikit-learn's Linear SVM\n",
        "print(f\"Accuracy of the scikit-learn Linear SVM: {accuracy:.2f}\")\n"
      ]
    }
  ]
}